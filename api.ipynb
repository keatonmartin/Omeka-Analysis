{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load API key from .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.getenv(\"API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Request items from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "URL = \"https://drvk.createuky.net/news-articles/api/items\"\n",
    "r = requests.get(URL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab descriptions from each item, clean them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "newline_pattern = re.compile(r'\\n+')\n",
    "html_pattern = re.compile(r'<.*?>')\n",
    "nbsp_pattern = re.compile(r'&nbsp;')\n",
    "\n",
    "def remove_tags(text):\n",
    "    no_newlines = newline_pattern.sub('', text)\n",
    "    no_tags = html_pattern.sub('', no_newlines)\n",
    "    return nbsp_pattern.sub(' ', no_tags)\n",
    "\n",
    "items = json.loads(r.text)\n",
    "\n",
    "cleaned_text = []\n",
    "for item in items:\n",
    "    for text in item[\"element_texts\"]:\n",
    "        if text[\"element\"][\"name\"] == \"Description\":\n",
    "            cleaned = remove_tags(text[\"text\"]).replace(\"\\n\", \"\")\n",
    "            if len(cleaned): cleaned_text.append(cleaned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports for frequency analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join every cleaned description into a string \n",
    "text = \" \".join(cleaned_text)\n",
    "\n",
    "# tokenize\n",
    "words = word_tokenize(text)\n",
    "cleaned = []\n",
    "\n",
    "# remove punctuation and stop words\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "custom_stopwords = []\n",
    "stopwords_list.extend(custom_stopwords)\n",
    "\n",
    "for word in words:\n",
    "    if word.isalpha() and word.lower() not in stopwords_list:\n",
    "        cleaned.append(word.lower())\n",
    "\n",
    "\n",
    "print(f\"Total number of words is {len(cleaned)}\")\n",
    "\n",
    "fdist = FreqDist(cleaned)\n",
    "\n",
    "for word, freq in fdist.most_common():\n",
    "    print(word, freq)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
